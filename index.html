<!DOCTYPE html>
<html lang="en'>
<title>W3.CSS</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="http://www.w3schools.com/lib/w3.css">
<head>
	<meta cahrset="utf-8" />
	<title>Im2Latex </title>
	<meta name = "viewport" content-"width-divice-width, intialscale=1.0">
	
	<style>
		body{
			margin: 0px;
			background: #E0E0E0
			font-family: Verdana, Tahoma, Arial, sans-serif;
			font-size: 18px;
			overflow: auto;
		}
		h1,h2,h3,h4{
			margin:0px;
			text-align:left;
			color: #9b4337;
		}
		h2{
			padding-left: 10px;
		}
		
		h3{
			padding-left: 20px;
		}
		h4{
			padding-left: 30px;
		}
		
		p{
			margin:0px;
			text-align: justify;
			padding: 2%;
			color: #AAAAAA;
		}
		
		#wrapper{
			margin: auto;
			max-width: 1200px;
			width: 98%;
			background: #202020;
			border: 1px solid #878E63;
			border-radius: 2px;
			box-shadow: 0 0 10px 0px rgba(12, 3, 25, 0.8);
		}
		
		img{
			display: block;
			margin:auto;
			height: auto;
			width: 1200px;
		}
		
		#callout{
			width: 100%;
			height: auto;
			background: #202020;
			overflow: hidden;
		}
		#callout p{
			text-align: right;
			font-size: 13px;
			padding: 0.1% 5px 0 0;
			color: #AAAAAA;
		}
		#callout p a{
			color:#AAAAAA;
			text-decoration: none;
		}
		header{
			width: 96%;
			min-heigh: 125px;
			padding: 5px:
			text-align: center;
		}
		nav ul{
			list-style: none;
			margin: 0;
			padding-left: 60px;
		}
		
		nav ul li{
			float:left;
			border: none;
			width: 20%;
		}
		nav ul li a {
			background: #202020;
			display: inline;
			padding: 5% 12%;
			font-weight: bold;
			font-size: 18px;
			color: #AAAAAA;
			text-decoration: none;
			text-align: center;
		}
		
		nav ul li a:hover, a{				
			background-color: #505050;
			color: #A0A0A0;
		}
		
		.banner img{
			width: 100%;
			border-top: 1px solid #878E63;
			border-bottom: 1px solid #878E63;
		}
		
		.leftcol{
			width: 55%;
			folat: left;
			margin:-2%	 1% 1%;
		}
		
		.sidebar{
			width: 40%;
			float: right;
			margin: 1%;
			text-align: center;
		}
		
		tab1{
			padding-left: 4em;
		}
		tab2{
			padding-left: 2em;
		}		
		tab3{
			padding-left: 1em;
		}
		
	</style>
	
	<head>
		<body>
			<div id="wrapper">
				<!-- <div id="VT">
				<p> Computer Vision</p>
				</div> -->
				
				<header>
					<img src="logo3.jpg" width="500" height ="auto" alt="Image to LaTex converter" title="Image to LaTex converter"/> 
				</header>
				<br>
				<nav>
					<ul>
						<li class = 'active'><a href = "#">About</a></li>
						<li><a href = "http://www.iapr-tc11.org/mediawiki/index.php/CROHME:_Competition_on_Recognition_of_Online_Handwritten_Mathematical_Expressions">Dataset</a></li>
						<li><a href = "https://github.com/KeerthiGowda/Im2LaTex/tree/gh-pages/Dataset">Image Dataset</a></li>
						<li><a href = "Im2Latex_Poster.pdf">Poster</a></li> 
						<li><a href = "https://github.com/KeerthiGowda/Im2LaTex/tree/gh-pages/canvas">Canvas</a></li>
					</ul>   
				</nav>
				
				<br>
				
				<section class "leftcol">
					<p>Typically most people find it comfortable to derive and think mathematics while writing either on paper or on a white-board. To be able to communicate these mathematical ideas with other people, it is beneficial to typeset it using standard tools like LaTex. However, transferring the content into this format can be cumbersome and need considerable human effort and time.<br><br>Hence, we propose to build an automated system that converts the mathematical equations in an image into LaTex using the techniques of computer vision and machine learning for pattern recognition</p>

				</section>
				
				<div class="banner">
					<img class="mySlides w3-animate-fading" src="Teaser_2.jpg" alt="Image to LaTex converter" title="Image to LaTex converter"/> 
					<img class="mySlides w3-animate-fading" src="Teaser_3.jpg" alt="Image to LaTex converter" title="Image to LaTex converter"/> 
					<img class="mySlides w3-animate-fading" src="Teaser_4.jpg" alt="Image to LaTex converter" title="Image to LaTex converter"/> 
					<img class="mySlides w3-animate-fading" src="Teaser_5.jpg" alt="Image to LaTex converter" title="Image to LaTex converter"/> 
				</div>
				
				<script>
					var myIndex = 0;
					carousel();

					function carousel() {
						var i;
						var x = document.getElementsByClassName("mySlides");
						for (i = 0; i < x.length; i++) {
						   x[i].style.display = "none";  
						}
						myIndex++;
						if (myIndex > x.length) {myIndex = 1}    
						x[myIndex-1].style.display = "block";  
						setTimeout(carousel, 5000);    
					}
				</script>
				
				<br>
				<h2><b> Block Diagram</b></h2>
					<div class="therapy">
						<img src="blockDiagram3.jpg" alt="Block Diagram" title="Image to LaTex converter"/> 
					</div>
				
				<p>
					<b>Perspective Correction</b><br>The image of the handwritten mathematical expression
						can be taken by the user in multiple angles. To achieve better accuracy, we need to
						correct the angle to obtain the frontal-view.<br><br>
					<b>Binarization</b><br>This corrected image needs to be binarized before passing it into the symbol classifier. This step is challenging because the method needs to be robust to
						uneven lighting conditions.<br><br>
					<b>Segmentation</b><br>Next, we need to process the binarized image and draw bounding boxes around each symbol.<br><br>
					<b>Symbol Classification</b><br> The segmented symbols are now passed into a symbol
						classifier that is trained using binary input images to output the correct symbols. We
						plan to compare SVM classifiers using traditional features like HOG and convolutional
						neural networks for this task.<br><br>
					<b> Structure Recognition</b><br> To be able to decode mathematical expressions, we need to understand the relation between adjacent symbols { for example, superscripts,
						subscripts, fraction and so on. Using these relations, we will be able to generate the
						latex code.<br><br>
						All the steps are discussed in detailed in the following sections.
						<br>
				</p>
				<div class="section">
					<h2><b>Pre-processing</b></h2>
					<img src="preProcessing2.jpg" alt="Block Diagram" title="Image to LaTex converter"/> 
					<p align="justify">
					<b>Canny edges:</b> Edges present in the image are found using canny edge detection technique. <br>
					<b>Hough transform:</b> The lines corresponding to the clip-board boundary are determined from these edges using Hough transformation.<br>
					<b>Boundary:</b> The four corner points are found by determining the intersection of the boundary lines. <br>
					<b>Homography:</b> The four points so determined are used to correct the perspective distortion using Homography.<br>
					<b>Binary image:</b> The image is then binarized with a threshold on intensity. <br>
					<b>Corners:</b> Harris features are detected on this image to obtain the cluster center of the text.<br>
					<b>Image cropping:</b> Image is cropped around this cluster center to get the text box. <br>
					<b>Connected components:</b> The text is segmented and each character is extracted from this text box<br></p>
				</div>
				
				<div class="section">
					<h2><b>Classification</b></h2>
					<img src="classification2.jpg" alt="Block Diagram" title="Image to LaTex converter"/> 
					<p>
					Canny edges: Edges present in the image are found using canny edge detection technique. 
					</p>
				</div>
				
				
				<div class="section">
					<h2><b>Details</b></h2>
						<tab3>Any classification problem has two steps, training and testing. We had trained the classifer for 57 symbols.
						<h3><b>Training</b></h3>
							<p> Following processes were done for training of the symbol dataset.
							</p>
						
							<h4><b>Data Collection</b></h4>
							<p>Chrome dataset consists of .inkml files. These files contains 
								the information of all the stokes in the symbol with their time stamp.

								<br><br>First step towards our data collection was to convert these inkml files to 
								images. We create a python script to extract the inkml data to a CSV file. This
								csv file consisted of all the strokes for a symbol and its labels. Using this CSV file, 
								we generated images using matlab. 

								<br><br>Dataset cleanup: Once we had the symbol images and their labels, we observed that 
								there were lot of errors in the dataset. Some of the symbols were misclassified, and some of them were junk data. Hence we cleaned up the dataset to have minimal training errors. All the images of the symbols and their corresponding labels have been uploaded under "Image Dataset" in this page. Also, we have uploaded the python script  and matlab code that was used to generate the data.
							</p>
							
							<h4><b>SVM</b></h4>
							<p>We train a linear SVM on conventional HOG features.</p>
							
							<h4><b>CNN</b></h4>
							<p>We train a Conv neural network[1] with three convolutional layers followed by two linear layers to perform a #class-way classification.We use dropout regularization[2] and conventional SGD to train the network. We use torch (hyper link) to train all models.</p>
							
							<h4><b>Place Holder</b></h4>
							<p>vajdovoiavjaio; </p>

						<h3><b>Testing</b></h3>
						<p>We tested in two ways, first using the training data and then on the real time data. We used 90% of the data to train the classifer and 10% of the data to test the accuracy of the classifier. The results of this testing is presented in the results section.<br>
						To do real time testing, we implemented the following two methods, <br>
						1. Webcam in the laptop to capture the image<br>
						2. Built an html canvas page where we can write the equation using mouse (source code of this is in the canvas tab). We created a custom sctipt to take a screen shot of the drawn image and send it to matlab.</p>

							
							
				</div>
				
				<div class="section">
					<h2><b>Results</b></h2>
					<p>
					Image was successfully converted to Latex code with levels of accuracy as follows:
					<br>Using SVM – 68.83 %
					<br>Using CNN - 92.19 %

					</p>
				</div>
				
				<div class="section">
					<h2><b>Comments / Feedback</b></h2>
					<p>
					Canny edges: Edges present in the image are found using canny edge detection technique. 
					</p>
				</div>
				
				<div class="section">
					<h2><b>Future work</b></h2>
					<p>
					Equation structure can be determined by using the relative location of bounding boxes in the pre-processed image.
					</p>
				</div>
				
				<div class="section">
					<h2><b>References</b></h2>
					<p>
					[1]	A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional
						neural networks. In Advances in Neural Information Processing Systems 25, pages
						1106–1114, 2012. <br>
					[2]	Dropout: A Simple Way to Prevent Neural Networks from Overfitting by Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.<br>
					
					[3] http://ivc.univ-nantes.fr/CROHME/datasets.php<br>
					[4] http://www.isical.ac.in/~crohme/CROHME_tasks2.html
					</p>
				</div>
				
				
				<footer>
					<div class="section"
						<p><b><tab3>Contact </b><br><tab3><b>Vikram Chandrashekar<tab1></tab1> Sujay Yadwadkar <tab1></tab1>Ashwin Kalyan<tab1></tab1>Keerthi Gowda</b><br>
						<tab3>vikramc@vt.edu <tab1></tab1><tab1></tab1><tab3></tab3> sujayr91@vt.edu <tab1></tab1><tab3></tab3> ashwinkv@vt.edu <tab2></tab2><tab3></tab3>keerthis@vt.edu
						</p>
					</div>

					<div class="section"
						<ul>
						<!--	<li><a href = "https://www.vt.edu"><img src="vt.jpg" alt="VT" title="Image to LaTex converter"/ class="VT_Logo" ></li> -->
						</ul>
					</div>
				</footer>
			</div>	
		</body>
	
	</head>
	
	


</html>
